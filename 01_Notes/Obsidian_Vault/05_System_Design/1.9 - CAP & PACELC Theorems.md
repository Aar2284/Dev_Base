
#chapter1

### _From “Foundations of System Design — A Technical Treatise”_

Distributed systems behave differently from single-machine systems because their components are separated by networks. Networks fail. Links drop packets. Nodes become unreachable without warning. Any system built across multiple machines must learn to function in this inherently unstable environment.

To reason about such systems, engineers rely on two conceptual tools: the **CAP theorem** and the **PACELC theorem**. Neither is a slogan nor a managerial catchphrase. They are analytical frameworks that expose the inherent limitations of distributed architectures.

This chapter explains them with precision.

---

## **1. The Context for CAP**

A distributed system—any system that stores or processes data on more than one machine—must coordinate its nodes.  
When one part of the system cannot contact another part, the system has experienced a **network partition**.

A partition is not a rare event. It is an unavoidable property of real networks.

Once a partition occurs, the system must make a choice. It must either:

- **Continue serving requests using the data it still has access to**, even if some updates are missing.
    
- **Refuse certain requests**, preserving strict correctness but sacrificing availability.
    

The CAP theorem formally defines this trade-off.

---

## **2. The CAP Theorem (Consistent, Available, Partition-Tolerant)**

The theorem states:

> _In the presence of a network partition, a distributed system cannot simultaneously provide both availability and strong consistency._

The operative phrase is **“in the presence of a partition.”**  
When all nodes can communicate, the system may offer both.  
When communication is disrupted, it must choose.

Let us define the terms precisely.

### **2.1 Consistency (Strong Consistency)**

Every read returns the most recent write.  
No stale data, no ambiguity, no divergence between replicas.

### **2.2 Availability**

Every request receives a valid response in a bounded amount of time.  
“Bounded” is important—an endlessly waiting client is not considered served.

### **2.3 Partition Tolerance**

The system continues operating despite arbitrary packet loss or node isolation.  
All real-world distributed systems must tolerate partitions because networks fail unpredictably.

---

## **3. The Three Resulting System Types**

Given a partition, a system can only be one of the following:

### **CP (Consistency + Partition Tolerance)**

- The system rejects some requests if it cannot guarantee correctness.
    
- Prioritizes accuracy over availability.
    
- Examples: ZooKeeper, etcd, HBase.
    

### **AP (Availability + Partition Tolerance)**

- The system continues serving requests even with incomplete information.
    
- Accepts temporary divergence between replicas.
    
- Examples: Cassandra, DynamoDB, Riak.
    

### **CA is not a real option**

A system can be consistent and available **only if it is not distributed**,  
or **only when the network never partitions**—a condition that does not exist at scale.

---

## **4. CAP’s Limitation**

The CAP theorem only describes system behavior **during a partition**.  
It says nothing about what happens during normal operation, when the network is functioning.

Engineers observed that many systems make trade-offs even when partitions are absent.  
This led to the refinement known as PACELC.

---

## **5. The PACELC Theorem**

Introduced by Daniel Abadi, PACELC expands CAP by addressing a simple, crucial question:

> _What does the system choose when there is no partition?_

The formula is written as:

**PACELC**

- **If a Partition occurs (P)** → the system must choose **Availability (A)** or **Consistency (C)**.
    
- **Else (E)**, when the system is healthy → it must choose between **Latency (L)** and **Consistency (C)**.
    

The second trade-off (latency vs consistency) is the one most engineers encounter daily.

---

## **6. Understanding the PACELC Model**

### **6.1 P → A or C (The original CAP choice)**

During a failure:

- AP systems prioritize response availability.
    
- CP systems prioritize correctness.
    

### **6.2 E → L or C (The everyday choice)**

When the network is functioning normally:

Systems choose between:

#### **Low Latency (L)**

Rapid responses, achieved through techniques such as:

- reading from the closest replica
    
- returning data before global coordination
    
- asynchronous replication
    

#### **Strong Consistency (C)**

Every read reflects the most recent write, enforced via mechanisms such as:

- synchronous replication
    
- leader-based commits
    
- quorum reads and writes
    

A system cannot achieve both minimal latency and strict consistency simultaneously.  
Strong coordination inevitably adds delay.

---

## **7. Examples of PACELC Classification**

### **Cassandra**

- **PA/EL**
    
    - During a partition → prioritizes Availability (A)
        
    - Else → prioritizes Latency (L) over strict Consistency (C)
        

### **DynamoDB**

- Similar to Cassandra: **PA/EL**
    

### **Google Spanner**

- **PC/EC**
    
    - During a partition → prioritizes Consistency
        
    - Else → continues enforcing consistency even at the cost of higher latency
        
    - Achieves global correctness via TrueTime clock synchronization
        

### **MongoDB (modern versions)**

- Configurable, but generally aligned with **PC/EC** when using majority write concerns.
    

---

## **8. What These Theorems Do Not Mean**

It is essential to dispel common misconceptions.

### **Not a ranking system**

CAP does not identify “good” or “bad” systems.  
It identifies constraints.

### **Not about performance in general**

CAP is about behavior under partition, nothing more.

### **Not mutually exclusive classes**

Many databases allow tunable consistency.  
The user decides the balance.

### **Not an excuse for vague architectural decisions**

Invoking CAP does not replace rigorous analysis of read-write patterns, latency budgets, or replication topology.

---

## **9. Why These Theorems Matter**

Distributed system design is a process of controlled compromise.  
CAP explains what happens when failure occurs.  
PACELC explains what happens when it does not.

Together, they provide the intellectual framework for:

- selecting appropriate databases,
    
- structuring replication policies,
    
- deciding on read/write quorum sizes,
    
- anticipating failure modes,
    
- evaluating the correctness-latency trade-off.
    

No engineer designing at scale can work without this mental model.

A system that ignores these principles will eventually reveal its negligence through data loss, inconsistency, or operational collapse.

---

## **End of Unit 1.9**

If you're ready, we will continue into **Unit 2** whenever you wish —  
or, if you prefer, we can move directly into **Unit 2.1** with the same book-like refinement.